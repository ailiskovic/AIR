{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fee8281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fee8281",
    "outputId": "fc19069e-4794-41aa-b867-cfdce2809c72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rodjo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960b9efb",
   "metadata": {
    "id": "960b9efb"
   },
   "outputs": [],
   "source": [
    "# load XML file \n",
    "xmlTree = ET.parse('Posts.xml')\n",
    "root = xmlTree.getroot()\n",
    "\n",
    "tags = {elem.tag for elem in xmlTree.iter()}\n",
    "xml_document = minidom.parse('Posts.xml')\n",
    "#xml_document = minidom.parse('/content/Posts.xml')\n",
    "row_elements = xml_document.getElementsByTagName('row')\n",
    "posts_elements = xml_document.getElementsByTagName('posts')\n",
    "\n",
    "#print(\"The amount of posts elements: \", len(posts_elements))\n",
    "#print(\"The amount of row elements: \", len(row_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85ff41f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c85ff41f",
    "outputId": "0af80141-52c4-4113-9b56-b06fc5c8cb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of arrtibutes segment of row element: \n",
      "{'Id': '1', 'PostTypeId': '1', 'AcceptedAnswerId': '3', 'CreationDate': '2010-09-13T19:27:46.227', 'Score': '15', 'ViewCount': '253', 'Body': '<p>Are questions that belong to the category \"What Apps Should I Get?\" appropriate Android Enthusiasts? How about if they\\'re asking for specific apps, such as \"What are good RSS aggregator apps that integrate with Google Reader and allow offline reading?\"</p>\\n', 'OwnerUserId': '36', 'LastEditorUserId': '440', 'LastEditDate': '2013-06-05T06:10:40.070', 'LastActivityDate': '2013-06-05T06:10:40.070', 'Title': 'Are \"What Apps Should I Get?\" Questions Appropriate?', 'Tags': '<discussion><faq><scope><questions>', 'AnswerCount': '10', 'CommentCount': '2', 'ContentLicense': 'CC BY-SA 2.5'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the prints which describe the structure of root element\n",
    "# the text is in a body attribute of a root elemnt\n",
    "\n",
    "row_element = root[0]\n",
    "print(\"The content of arrtibutes segment of row element: \")\n",
    "print(row_element.attrib)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f8eb7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5f8eb7c",
    "outputId": "1f3e7dcd-62bf-43b3-b19a-fd666007a70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are questions that belong to the category \"What Apps Should I Get?\" appropriate Android Enthusiasts? How about if they're asking for specific apps, such as \"What are good RSS aggregator apps that integrate with Google Reader and allow offline reading?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = row_element.attrib['Body']\n",
    "cleantext = BeautifulSoup(string, features=\"html.parser\").text\n",
    "# print example of old and new text\n",
    "print(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c15859e",
   "metadata": {
    "id": "7c15859e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "\n",
    "for string in root.iter('row'):\n",
    "    body = string.attrib['Body']\n",
    "    cleantext = BeautifulSoup(body, features=\"html.parser\").text\n",
    "    list.append(cleantext)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6de8e2",
   "metadata": {
    "id": "de6de8e2"
   },
   "outputs": [],
   "source": [
    "# now we need to delete hyperlinks, both http and https\n",
    "for i in range(len(list)):\n",
    "    list[i] = re.sub(r'http\\S+', '', list[i])     \n",
    "\n",
    "# remove new lines    \n",
    "for i in range(len(list)):\n",
    "    list[i].replace(\"\\n\", \"\")    \n",
    "    \n",
    "# apply tockenizer to split words to elements    \n",
    "for i in range(len(list)):\n",
    "    word_lower = list[i].lower()\n",
    "    cleaned_word = tokenizer.tokenize(word_lower)\n",
    "    list[i] = cleaned_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709d651",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4709d651",
    "outputId": "fca57ac6-a751-405a-bf6d-37d08fa8a684"
   },
   "outputs": [],
   "source": [
    "# track the current progress\n",
    "progess = 0\n",
    "\n",
    "# Less documents, less execution time\n",
    "list = list[:100]\n",
    "\n",
    "for i in range(len(list)):\n",
    "    list[i] = [word for word in list[i] if not word in stopwords.words()]\n",
    "    progess += 1\n",
    "    if progess % 100 == 0: \n",
    "        print(progess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da7d99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54da7d99",
    "outputId": "cb8078be-b041-4556-87c7-72aad9998bfb"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(list, min_count = 1, window = 5, vector_size=20)   # newer gensim versions, vector_size == size\n",
    "#model = gensim.models.Word2Vec(list, min_count = 1, window = 5, size=20)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QfRwA1bxrxwg",
   "metadata": {
    "id": "QfRwA1bxrxwg"
   },
   "outputs": [],
   "source": [
    "dictionary = dict({})\n",
    "#for idx, key in enumerate(model.wv.vocab):  #< 4 version\n",
    "for idx, key in enumerate(model.wv.key_to_index):\n",
    "    dictionary[key] = model.wv[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JQHXJKnlsE8g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQHXJKnlsE8g",
    "outputId": "324dc26b-4d82-473e-cbc3-a3ba99f99548"
   },
   "outputs": [],
   "source": [
    "print(type(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJB7rP2HsTNg",
   "metadata": {
    "id": "OJB7rP2HsTNg"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The method shoud expand the currenty query based on the cosine similartiy based on the given threshold. \n",
    "The method should return the list of elements, including the expending ones (if any). \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "dictionary : dictionary\n",
    "    Dictionary based on the vec2word model \n",
    "original_query : str\n",
    "    The query\n",
    "number_of_elements_to_expand: int\n",
    "    Number of elements to expand\n",
    "threshold : float, optional\n",
    "    The similarity threshold required to add word to a query (default = 0.00)\n",
    "\"\"\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity   # add import to beginning\n",
    "import numpy as np\n",
    "\n",
    "def cossim(query_vectors, doc_vectors):\n",
    "    similarities = []\n",
    "    for query_vector in query_vectors:\n",
    "        query_sim = []\n",
    "        for doc_vector in doc_vectors:\n",
    "            similarity = cosine_similarity(query_vector.reshape(1, -1), doc_vector.reshape(1, -1))\n",
    "            #print(similarity[0][0])\n",
    "            query_sim.append(similarity[0][0])\n",
    "        similarities.append(query_sim)\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def queryExpansionAlgorithm(dictionary, original_query, number_of_elements_to_expand, threshold=0.00):\n",
    "    query_tokens = original_query.split()\n",
    "    query_vectors = [model.wv[token] for token in query_tokens]\n",
    "    # 1) add preprocessing steps in code above to handle tokens that are not in our current model. OR handle with different idea\n",
    "    # 2) add error handling in case token is not in model -> ignore this part of query for example = remove token\n",
    "    #print(query_vectors)\n",
    "    #print(dictionary.values())\n",
    "    similarity_matrix = cossim(query_vectors, dictionary.values())\n",
    "\n",
    "    dict_keys_list = []\n",
    "    for key in dictionary.keys():\n",
    "        dict_keys_list.append(key)\n",
    "\n",
    "    expanded_query_elements = []\n",
    "    for i, query_word in enumerate(query_tokens):\n",
    "        # top n highest similarity\n",
    "        top_n = np.argpartition(similarity_matrix[i], -number_of_elements_to_expand)[-number_of_elements_to_expand:]\n",
    "        #rint(top_n)\n",
    "\n",
    "        # Check if the cosine similarity between each of the top N words and the original word is above the threshold\n",
    "        for j in top_n:\n",
    "            if similarity_matrix[i][j] >= threshold:\n",
    "                expanded_query_elements.append((dict_keys_list[j], similarity_matrix[i][j]))\n",
    "    \n",
    "    return expanded_query_elements\n",
    "    \n",
    "    \n",
    "print(queryExpansionAlgorithm(dictionary, \"code developers\", 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2af61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
